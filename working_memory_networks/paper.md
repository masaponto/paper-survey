# Working Memory Networks: Augmenting Memory Networks with a Relational Reasoning Module 
- https://arxiv.org/abs/1805.09354
- Github: https://github.com/jgpavez/Working-Memory-Networks

## 概要
- 近年Deep Learning を用いた複雑な関係推論を解く手法が提案されている。
- 関係推論を行うため、Memory Networkは外部メモリとAttention機構を組み合わせた手法である。
- しかしながら、これらのアーキテクチャはより複雑な関係推論は困難である。
- 一方でRelation Networks (RNs)は関係推論のタスクにおいて飛び抜けた結果を出した。
- 残念なことに、このRNは複雑な問題においては、メモリの使用量など大きな計算コストが2次的にに上がってしまう。(オブジェクト同士の全組み合わせを計算するため)
- この問題を解決するために、我々はWorking Memory Network (MemNN アーキテクチャ)を提案する。そしてそれはワーキングメモリとRNモジュールを組み合わせたものである。
- 本手法はRNsを導入しているが、計算コストの増加を二次的でなく線形的に削減する。
- 我々は本モデルをbAbIデータセットとvisual QAデータセットのNLVRで検証した。
- 結果、MSEが0.5%以下でSOTAを達成。
- さらに、我々のシンプルなモデル2つのアンサンブルすることで、joint版(1つのモデルで複数のタスクを学習させる方針)のベンチマーク20タスクすべてを解く事に成功した。

## Intro
日常生活のタスクを解決するために必要な中心的な機能は複雑な関係推論である。
それは、環境を理解、表現し、過去の経験から学習し、蓄えられた情報から問題を解決するということである。
私達人間のそれらの問題を解く能力は、複数の特別な機能からできています。(短期記憶、長期的意味記憶、手続き記憶、そしてそれらの記憶を操作するAttention機構。)

近年、複雑な関係推論を行うニューラルネットワークが多数提案されています。
Deep Learningは複雑な(関係)推論のためのシンボル的アプローチではなく、知覚情報からの表現を学習することができます。
そのため、Deep Learningはシンボルグラウンディング問題に直面することなく、シンボルを分類していくアプローチより、良い一般化が可能です。
(シンボルグラウンディング問題とは記号システム内のシンボルがどのようにして実世界の意味と結びつけられるかという問題)
これらのNeural Network の多くのモデルは、メモリ(記憶装置)や、Attention機構を使います。
Memory network, dyanamic memory network, Neural Tuning Machine (NTM) は 入力からのメモリと学習されたAttention機構によるメモリアクセスからなる。
いくつかのメモリの記憶にAttention機構によって注意が向けられた後、複数ステップの手続きにより、注意が向いた記憶は(入力データと)結合され、最終結果を出力する出力層に流れます。
これらのいくつかのプロセスの間に、networkは複雑な推論情報メカニズムが欠如してしまう。
そしてそれは、エンティティ(オブジェクト)の関係を推論(Relational Reasoning)するようなタスクにおいて必要な情報である。
一方で、Santoroによって提案されたRelational Network(RNs)は関係推論において高い性能が確認されました。
Relational Networkの欠点はそれぞれ入力オブジェクトのペア(二乗的な組み合わせ関係)を考えなくてはならないこと。
この欠点は、複雑なタスクに対してへの適用可能性に制限をかけてしまうし、また、順伝播、逆伝播に高い計算コストがかかる。
この問題を解決するために、memory networkアーキテクチャのひとつである、Working Memory Networks (W-MemNN)を提案する。
我々はオリジナルのMemory NetworkにRNモジュールと新しいワーキングメモリバッファを追加した。

このMemory NetworkのAttention機構は入力の重要でない部分をフィルタリングすることができる。
そしてそれは、RNの関係推論の機能を保ちつつ、計算コストを削減することができます。
このW-MemNNは3つのコンポーネントから構成されます。
- 1) Input module, 知覚的な入力ベクトルを内部表現ベクトルに変換します。そして、Short-term storage(短期記憶装置)に保存します。
- 2) Attention controller, 上記の内部表現にAttention機構を適用します。そして、working memroy bufferを更新します。
- 3) reasoning module, working memory に保存されたオブジェクトの集合を扱います。そして、最終結果を出力します。

このコンポーネントベースのアーキテクチャは、BaddeleyとHitchが提案したmulti-component working呼ばれる
コグニティブサイエンス(認知科学)における有名なモデルを元に触発されました。
私達はこの提案モデルを bAbIというテキストベースのQAタスクのデータセットを使って検証しました。
bAbIは 異なる関係推論スキルを測る事ができる20の問タスクから構成されております。
HenaffのEntNetと呼ばれる手法がタスクごとに異なるモデルを適用していくこと(pertask training version)にフォーカスしていたことに対し、
本件では、同時にすべてのタスクを学習する方針(jointly trained version)にフォーカスする。
bAbI-10k を (jointly train)学習してSOTAを達成しました。前のSOTAと比べて2%以上精度が向上しました。
シンプルに2モデルでのアンサンブルをした結果20タスクをすべて正解することができました。
また、NLVRと呼ばれるVisual QA taskでもテストしてみました。結果、AndresのModule Neural Netwokrsのレベルの精度がでました。
しかしながら、私達のモデル性能はModule Networkに生データをそのまま入力した時と同じ性能でした。(追加のtext処理はなし)

最後に、質と量の解析結果によると、本提案のRelationalNetworkの導入は、関係推論を含むタスクにおいて、MemNNの決定的な精度向上させることを示しました。
また、かなりの計算時間の削減も達成しております。
この結果から、RNがより複雑な大きいタスクに適用できるようになると私達は考えております。

## Model
私達のモデルはmemory network アーキテクチャに基づいています。
memory network ではなく、より複雑なタスクをとくために、reasoning module を導入しております。
提案手法は3つのモジュールをもっています、入力モジュール、Attentionコントローラ、関係推論モジュールです。
入力情報は複数のパスやhopで処理されます。
それぞれのパスで前のhopの出力は、現在のパスの状態をだんだんと加算しながら作ることができます。

![figure_1](https://github.com/masaponto/paper-survey/blob/master/working_memory_networks/figure_1.png)


#### Inputモジュール
input モジュールはは知覚入力を内部特徴表現へ変換します。
入力情報はチャンク毎に処理されます。それぞれのチャンクは短期記憶装置に保存されます。チャンクの定義はタスクに依存します。
テキストのQAタスクでは、私達はそれぞれのチャンクを文章と定義しました。他のオプションとしてn-gramや文章全体が考えられます。
この短期記憶装置はhopの間だけアクセスされます。

#### Attention controller
Attention コントローラはモデルのメモリバッファのどの部分に注意を向けるかを決定します。
注意が向けられたメモリバッファはworking memoryバッファに全てのホップの間保持されます。
Attentionコントローラは手元のタスクによって状態が決定します。
例えば、QAタスクでは、Questionはattentionを決定します。
また、attentionは前のホップの出力によっても決定されます。そしてそれは、モデルに時間とともに記憶の新しい部分に注目を変えさせます。
多くのモデルはメモリバッファとquestionの間で互換性関数を使いながらそれぞれのメモリバッファのためにAttentionを計算します。
それから、その出力は、attentionを重みとしたメモリバッファの重み付き和で計算されます。
それぞれのメモリバッファのAttentionの計算するシンプルな方法はdot-product attentionです。
この種のメカニズムはオリジナルのメモリネットワークで使われます。そしてそれぞれのメモリバッファと質問で　dot-productとしてAttentionの値を計算します。
けれども、この種のattenitonはシンプルです。より複雑なタスクにいおては十分でないかもしれない。
また、Atteniton機構において学習された重みがない場合、Attenitonは学習されたembeddingに全体的に依存します。
入力モジュールとattentionモジュールの学習を分割するために私達が避けたかったことです。
dot-product Attentionで学習する1つの方法はメモリバッファとqueryベクトルを線形に写像することです。
学習された写像のための行列によってベクトルの積をとることでなされます。(ちなみにフィードフォワードニューラルネットワークとおなじです)
このようにして、私達はAttenitonと入力のembeddingの学習をそれぞれの部分で定義することができます。そして、より複雑なAttenitonのパターンを使用ができます。

#### Reasoning module
ワーキングメモリバッファに保存されたメモリはreasoning moduleに通されます。
関係推論メカニズムの選択はまだ結論がでていなく手元のタスクに依存します。
今回はRelational network(RN)を関係推論モジュールとして使いました。
RNはメモリの間の関係を推論するためにAtentionされたメモリのペアをうけとります。
これは役に立ちます。例えば、それは比較も含まれているからです。
モデル全体の詳細な説明は画像1に示されています。

### テキストQAタスクのためのW-MemN2N
私達はテキストQAタスクのためのモデルの実装の説明に進みます。
テキストQAタスクにおいて、入力は文章または要素、質問そして答えから構成されます。
ゴールは与えられた要因に基づいて正しく質問に答えることです。
(s, q, a)を入力サンプルを表します。文の集合s = {xi} (i=1 .. M), とクエリqと答えa。
それぞれの文はM個のword含みます。{wi} (i=1 .. M)。
ここで、それぞれのワードはベクトルの大きさ大きさ|V|のone-hotベクトルで表現されます。(|V|は辞書の大きさになります。)
質問は入力文章として表現されているQ個の単語でできおります。

#### Input module
それぞれの文章のそれぞれの単語はembedding行列を使ってベクトル表現 vi にエンコードされます。  
W ∈ R^ |V |×d  
ここでdはembedding sizeです。
そのとき、文章はGRUの最終出力を使ってメモリベクトルmi に変換されます。  
mi = GRU([v1, v2, ..., vM])
それぞれのメモリ  
{mi} i = 1..L where mi ∈ R^d  
は短期メモリバッファに保存されます。
questionの文章はGRUの最終出力を使ってベクトルuに同じ方法でエンコードされます。

#### Attention controller
私達のアテンションモジュールはVaswaniに提案されたMulti-head attention(2017)メカニズムにもとづいてます。
最初に短期メモリバッファのメモリは写像の行列で写像されます。  
Wm ∈ R(d*d) mi = Wm*mi  
そして、写像されたメモリと質問ベクトルの類似度はScaled dot product アテンションによって計算されます。
次に、メモリはattentionの重みαによって出力を得るために計算されます。h=sum(j) α*m.
マルチヘッドアテンションメカニズムでは メモリはS回異なる写像行列(Wm s=1...S)で写像されます。
それぞれの写像されたメモリのグループで、出力ベクトルh(i=1toS)はScaled dot product アテンションによって計算されます。
最後に全ての出力ベクトルは異なる行列を使って再び結合(concat)されて写像されます。  
Ok = [h1; h2; h3; ... hs] * Wo.  
ここで ; は結合の演算子です。そいて Wo ∈ R ^ {Sd * d}.
ベクトルOkはk番目のhopの最終返り値のベクトルです。
このベクトルはワーキングメモリバッファに保存されます。
アテンション機能は複数回繰り返すことができます。
それぞれのhopでアテンションは前のhopの出力に質問ベクトルuは置き換えられることによって状態が作られることができます。
このため、私達は単純なニューラルネットワークのft(x)で得られた出力を使います。
そして私達は新しいconditinersとしてニューラルネットワークの出力を使います。  
Ok = ft(ok).  
このネットワークはhopの間の移り変わりのパターンのいくつかの学習を許容します。
私達はマルチヘッドアテンションはjoint bAbIのタスクにおいてとても役に立つということを見つけました。
これはbAbIデータセットの固有の複数タスクがもつ要因がさせることができます。
ぞれぞれアテンションヘッドが関係タスクの異なるグループに適応されることでなりたっている可能性があります。
しかしながら、私達はこれ以上調べませんでした。
また、このセクションではあたしたちはそれぞれのhopで同じメモリの集合使います。これは必ず必要であるわけではありません。
より長いシーケンスではそれぞれのhopは入力シーケンスの異なる部分を操作することができます。このことは、いろいろなステップで入力の処理をゆるします。

#### Relational Network
ワーキングメモリバッファに保存された出力は関係推論モジュールに通されます。
関係推論モジュールは、この論文では、relation networkを使います。
RNの出力ベクトルは質問ベクトルと一緒にペアに結合されます。
それぞれのペアはニューラルネットワークgθにと通されます、そして全てのニューラルネットワークの出力は一つのベクトルを生成します。
そして、その合計が最後のニューラルネットワークfφに通されます。  
r = fφ(sigma gθ([oi, oj, u]))  
その時Relation netwokの出力は、最後の重み行列とsoftmax関数を通ります、そして推論された答えを生成します。  
a = softmax(Vr)  
ここで V ∈ R |A|×dφです。
A は解答可能な数字です。そしてdφはfφの出力の次元数です。
ネットワーク全体は出力a^と真のラベルaの間をend-to-endに標準的な交差エントロピー誤差で学習されます。

## Related work
## Experiments
### Textual Questios Answering
テキストのQAタスクでの私達のモデルを評価するために、私達はFacebook bAbI-10k dataset を使いました。
bAbIデータセットは20の異なるタスクをもつテキストQAタスクのベンチマークデータセットです。
それぞれのタスクは異なる関係推論技術をテストするために作成されました。例えば、演繹法、帰納法そして、会議解決。
そのタスクのいくつかは関係推論が必要です。例えば、異なるエンティティの大きさを比較する事。
それぞれのサンプルは質問と答え、そして要因の集合を持ちます。
データセットのサイズに応じて、2種類のデータセットがあります。bAbI-1k と bAbI-10kです。
この論文では、私たちは、各タスクごとに10,000個の学習データをもつbAbI-10k版に着目します。
一つのタスクは95%以上の正解率を達成したら解けたと判定します。
学習はpertask(それぞれのタスクごとに行う)か、joint(全てのタスクを同じモデルで行う)ことができます。
Entnet(Henaff etal., 2016)やいくつかのモデル(Liu and Perez, 2017)はper-task学習の精度に着目しております。
私達はjointでの学習に着目します。なぜなら、私達の考えは、モデルの一般性をよりみたいからです。
データセットの詳細な解析は Lee et al (2015)で見ることができます。
### Model Details
入力データをエンコードするために、私達は1文のそれぞれの単語を大きさdの実数のベクトルに写像するword embeddingを使いました。
私達はd030と定義しました。そして、それぞれの文を処理するために30unitsのGRUを使いました。
私達は質問の直前にサポート集合の30文を使いました。
質問は異なるGRUで同じ形状で使いながら処理されました。
私達は、multi-headアテンションでは8ヘッド使いました。
それぞれのhopの出力を操作する変換ネットワーク(ft)では、私達は15と30の隠れニューロンをもつ2層のMLPを使った。
(つまり出力はメモリの次元数を保存する。)
私達はH=4ホップを使った
(また、ワーキングメモリバッファが4であることと等しい)
関係推論uモジュールでは、私達はgθにそれぞれのレイヤに128ユニットとReLUをもつMLPを使った
私達はfφネットワークを除きました、なぜなら、私たちは、それを使ったとき、改善がみられなかったから。
最後のレイヤーは答えの語彙をだすsoftmaxのロジットを仏録する線形レイヤーです。

### Training Details
私達は私達のモデルをクロスエントロピー誤差とAdam最適化(Kingma and Ba, 2014)を使って学習を行いました。
私達は学習率ν = 1e−3を使いました。
私達は400エポックで学習しました。
Sukhbaatar et alのようにバッチサイズは32で行いました。
私達は、バッチ処理で、lossの平均をしませんでした。
また、私達は40以上のノルムではclipph gradientsをしました。(Pascanu et al.,2013)
全ての全結合層で、L2正規化を1e-3の値で行った。
全ての重みはGlorot標準初期化(Glorot and Bengio, 2010)を使って初期化しました。
学習データセットの10%はアーキテクチャを選定するため、またハイパーパラメータチューニングのためのvalidation setにしました。
いくつかのケースで、私達は学習率が1e-5より低くすることと、
5エポックごとに20エポックまでν/2 にアニーリングして、400エポックの後再学習することはいい精度がでることを見つけた。

### Memory Augmented Neural Network

## WHAT’S THE CORE IDEA OF THIS PAPER?

## WHAT’S THE KEY ACHIEVEMENT?

## Experiment

## Future works

## Possible Business Applications

## WHAT ARE POSSIBLE BUSINESS APPLICATIONS?
